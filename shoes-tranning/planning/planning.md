# Plano de Desenvolvimento: Modelo de Gera√ß√£o de Dados Sint√©ticos para E-commerce de Moda

## 1. Vis√£o Geral do Projeto
### 1.1 Objetivo
Desenvolver um sistema de gera√ß√£o de dados sint√©ticos para produtos de moda (e-commerce) que seja capaz de:
- Gerar imagens realistas de produtos
- Criar descri√ß√µes textuais coerentes
- Produzir metadados estruturados consistentes
- Manter correspond√™ncia multimodal entre imagem, texto e metadados

### 1.2 Dataset Base
- **Fonte**: Fashion Product Images Dataset (Kaggle)
- **Localiza√ß√£o**: `/Users/jwcunha/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1/fashion-dataset/fashion-dataset/`
- **Estrutura**:
  - `styles.csv`: Mapeamento de produtos com categorias (44,446 registros)
  - `images/`: Imagens de alta resolu√ß√£o (44,441 arquivos .jpg)
  - `styles/`: Metadados completos (44,446 arquivos .json)
  - `images.csv`: Mapeamento de imagens com URLs originais
- **Tamanho Total**: ~30GB
- **Caracter√≠sticas**: Imagens profissionais + atributos manuais + descri√ß√µes textuais

### 1.3 An√°lise Inicial do Dataset (REALIZADA)

> **Nota**: Esta se√ß√£o documenta os insights obtidos da an√°lise explorat√≥ria inicial realizada em 2025-10-26.
> Scripts de an√°lise dispon√≠veis em: `shoes-tranning/exploratory/`

#### 1.3.1 Estat√≠sticas Gerais
- **Total de Produtos**: 44,446 produtos √∫nicos
- **Per√≠odo Temporal**: 2010-2017 (8 anos de dados)
- **Integridade dos Dados**:
  - ‚úì Baixa taxa de valores faltantes
  - ‚úì Sem registros duplicados
  - ‚ö†Ô∏è 5 imagens faltantes (~0.01%)
  - ‚úì Alta completude de metadados JSON

#### 1.3.2 Distribui√ß√£o por Categorias Principais

**Master Categories** (3 principais):
1. **Apparel**: ~31,000 produtos (70% do dataset)
2. **Footwear**: ~8,500 produtos (19%)
3. **Accessories**: ~4,000 produtos (9%)
4. Personal Care, Free Items, Home: <2% cada

**Top 10 Article Types** (mais representados):
| Rank | Tipo de Artigo | Quantidade | % do Dataset |
|------|----------------|------------|--------------|
| 1 | Tshirts | ~6,200 | 14.0% |
| 2 | Shirts | ~5,700 | 12.8% |
| 3 | Casual Shoes | ~2,500 | 5.6% |
| 4 | Watches | ~2,200 | 5.0% |
| 5 | Sports Shoes | ~1,900 | 4.3% |
| 6 | Kurtas | ~1,600 | 3.6% |
| 7 | Tops | ~1,500 | 3.4% |
| 8 | Handbags | ~1,450 | 3.3% |
| 9 | Sunglasses | ~1,400 | 3.1% |
| 10 | Jeans | ~1,350 | 3.0% |

*Top 10 cobrem ~57% do dataset total*

**Distribui√ß√£o por G√™nero**:
- Men: ~60%
- Women: ~38%
- Boys/Girls/Unisex: ~2%

**Distribui√ß√£o por Esta√ß√£o**:
- Summer: ~34%
- Fall: ~26%
- Winter: ~21%
- Spring: ~19%
(Distribui√ß√£o relativamente equilibrada)

**Distribui√ß√£o por Uso**:
- Casual: ~76%
- Formal: ~11%
- Sports: ~8%
- Ethnic: ~4%
- Party/Smart Casual: <1% cada

**Top 10 Cores**:
1. Black (~8,500 produtos)
2. White (~5,000)
3. Blue (~4,500)
4. Grey (~3,000)
5. Navy Blue (~2,500)
6. Red (~2,000)
7. Brown (~1,800)
8. Pink (~1,500)
9. Green (~1,400)
10. Purple (~1,200)

#### 1.3.3 Caracter√≠sticas das Imagens

**Resolu√ß√µes Comuns**:
- Varia√ß√£o significativa de resolu√ß√µes (n√£o padronizadas)
- Resolu√ß√µes t√≠picas: 80x60, 60x80, 2400x3200
- Aspect ratios predominantes: ~0.75 (retrato)
- Maioria das imagens em modo RGB

**Qualidade Visual**:
- ‚úì Fotografias profissionais de produtos
- ‚úì Fundo limpo/branco (excelente para treinamento)
- ‚úì Boa ilumina√ß√£o e enquadramento consistente
- ‚úì Produto centralizado e em destaque

**Tamanho de Arquivos**:
- Varia√ß√£o: ~10 KB a ~500 KB por imagem
- M√©dia: ~150 KB
- Formato: JPEG com compress√£o vari√°vel

#### 1.3.4 Metadados JSON - Estrutura Rica

**Campos Principais** (presentes em >90% dos registros):
- `id`, `productDisplayName`
- `gender`, `masterCategory`, `subCategory`, `articleType`
- `baseColour`, `season`, `year`, `usage`
- `brandName`, `brandUserProfile`
- `price`, `discountedPrice`
- `styleImages` (m√∫ltiplas resolu√ß√µes)

**Descri√ß√µes de Produtos**:
- **productDescriptors.description**: Descri√ß√£o t√©cnica do produto
  - M√©dia: 15-30 palavras
  - Formato: HTML (requer limpeza)
  - Conte√∫do: Caracter√≠sticas f√≠sicas, materiais, detalhes

- **productDescriptors.style_note**: Nota de estilo
  - M√©dia: 30-50 palavras
  - Formato: HTML
  - Conte√∫do: Como usar, ocasi√µes, combina√ß√µes

- **productDescriptors.materials_care_desc**: Materiais e cuidados
  - M√©dia: 10-20 palavras
  - Informa√ß√µes sobre composi√ß√£o e manuten√ß√£o

**Atributos Adicionais** (articleAttributes):
- Fit (Regular, Slim, Loose, etc.)
- Fabric (Cotton, Polyester, Leather, etc.)
- Occasion (Casual, Formal, Party, etc.)
- Neck (Round, V-Neck, Collar, etc.)
- Pattern (Solid, Striped, Printed, etc.)
- Sleeve (Short, Long, Sleeveless, etc.)

**Informa√ß√µes de Pre√ßo**:
- Faixa de pre√ßos: ‚Çπ100 - ‚Çπ50,000+
- M√©dia: ~‚Çπ2,000-‚Çπ3,000
- Desconto m√©dio: 10-15%

**Marcas**:
- Total de marcas √∫nicas: ~2,000+
- Top marcas: Nike, Puma, Adidas, Roadster, United Colors of Benetton
- Mix de marcas premium e acess√≠veis

#### 1.3.5 An√°lise Temporal

**Distribui√ß√£o por Ano**:
- 2010-2012: Crescimento gradual
- 2011-2012: Pico de produtos adicionados (~15,000-18,000/ano)
- 2013-2015: Estabiliza√ß√£o
- 2016-2017: Menor volume

**Implica√ß√µes**:
- Dataset cobre tend√™ncias de moda de 8 anos
- Poss√≠vel usar temporal conditioning na gera√ß√£o
- Estilos vintage (2010-2012) vs modernos (2016-2017)

#### 1.3.6 Insights Multimodais

**Consist√™ncia Imagem-Metadados**:
- ‚úì Cores base geralmente correspondem √†s cores vis√≠veis nas imagens
- ‚úì Categorias (articleType) alinhadas com o produto fotografado
- ‚úì G√™nero consistente com estilo visual do produto

**Descri√ß√µes vs Imagens**:
- Descri√ß√µes mencionam caracter√≠sticas vis√≠veis
- Style notes complementam com contexto de uso
- Oportunidade para valida√ß√£o multimodal durante gera√ß√£o

#### 1.3.7 Desafios Identificados

1. **Varia√ß√£o de Resolu√ß√£o**: Imagens n√£o padronizadas
   - Solu√ß√£o: Redimensionamento e normaliza√ß√£o no pr√©-processamento

2. **HTML em Descri√ß√µes**: Tags HTML nos textos
   - Solu√ß√£o: Limpeza com regex antes do treinamento

3. **Desbalanceamento de Categorias**: Algumas categorias com <100 exemplos
   - Solu√ß√£o: Focar em top 20-30 categorias para MVP

4. **Imagens Faltantes**: 5 produtos sem imagem
   - Impacto: Neglig√≠vel (<0.01%)

5. **Atributos Opcionais**: Nem todos os produtos t√™m todos os atributos
   - Solu√ß√£o: Tratar campos faltantes como "n√£o especificado" ou usar valores padr√£o

#### 1.3.8 Oportunidades Estrat√©gicas

1. **Categorias Priorit√°rias para MVP** (ATUALIZADO):
   - **Fase 1 (ATUAL)**: Casual Shoes (~2,845 exemplos)
     - Justificativa: Categoria com boa quantidade de exemplos
     - Fundo limpo facilita treinamento de modelos generativos
     - Varia√ß√£o controlada: cores, materiais, estilos
     - 3¬™ categoria mais popular do dataset
   - Fase 2 (FUTURO): Tshirts (~7,067), Shirts (~3,217)
   - Fase 3 (FUTURO): Top 20 categorias

2. **Estrat√©gia de Desenvolvimento Incremental**:
   - **Sprint 1-2**: Gera√ß√£o de imagens sint√©ticas (Casual Shoes)
   - **Sprint 3**: Expans√£o do dataset com imagens geradas
   - **Sprint 4**: Valida√ß√£o e m√©tricas de qualidade
   - **Sprint 5+**: Gera√ß√£o de texto e metadados (futuro)

3. **Vantagens do Dataset**:
   - ‚úì Tamanho suficiente para fine-tuning (~44K produtos)
   - ‚úì Diversidade de categorias, cores, estilos
   - ‚úì Metadados ricos para condicionamento
   - ‚úì Descri√ß√µes textuais para training de LLMs (futuro)
   - ‚úì Imagens de alta qualidade profissional

4. **Gera√ß√£o Condicional - Casual Shoes**:
   - Condicionamento por: cor, g√™nero, marca
   - Controle de estilo: esportivo vs casual formal
   - Varia√ß√£o de materiais: couro, canvas, sint√©tico
   - √Çngulos e poses consistentes

---

## 1.4 Especifica√ß√µes de Hardware e Estrat√©gia de Treinamento Local

### 1.4.1 Hardware Dispon√≠vel
**Sistema**: Mac Studio (2023)
- **Processador**: Apple M2 Max
- **Mem√≥ria**: 32 GB RAM unificada (compartilhada entre CPU e GPU)
- **GPU**: Integrada M2 Max (38-core)
- **Backend**: PyTorch com Metal Performance Shaders (MPS)

**Capacidades de Treinamento**:
- ‚úì Fine-tuning de modelos at√© ~13B par√¢metros (com quantiza√ß√£o)
- ‚úì Stable Diffusion XL e SD 1.5 com LoRA
- ‚úì Batch sizes modestos (2-8 dependendo do modelo)
- ‚úì Mixed precision training (fp16/bf16)
- ‚ö†Ô∏è Mem√≥ria compartilhada: ~20-24GB dispon√≠veis para modelos ap√≥s overhead do sistema

### 1.4.2 Modelos Otimizados para Apple Silicon

#### Gera√ß√£o de Imagens - Stable Diffusion com LoRA
**Modelo Recomendado**: Stable Diffusion 1.5 ou SDXL com LoRA fine-tuning

**Justificativa**:
- SD 1.5: ~4GB VRAM, treina bem em M2 Max
- SDXL: ~8-10GB VRAM com optimiza√ß√µes, melhor qualidade
- LoRA reduz mem√≥ria de treinamento em 90%
- Suporte nativo MPS no PyTorch 2.0+

**Configura√ß√£o de Treinamento**:
```python
# Stable Diffusion 1.5 + LoRA
Model: runwayml/stable-diffusion-v1-5
LoRA rank: 8-16 (menor = menos mem√≥ria)
Learning rate: 1e-4
Batch size: 2-4 (com gradient accumulation)
Resolution: 512x512
Training steps: 3000-5000
Mixed precision: fp16 (MPS)

# Otimiza√ß√µes M2 Max
- Use torch.mps.empty_cache() entre epochs
- Gradient checkpointing: True
- Gradient accumulation: 4-8 steps
- CPU offload para componentes n√£o cr√≠ticos
```

**Estimativa de Tempo**:
- ~2-4 horas para 3000 steps (categoria Tshirts ~6200 imagens)
- ~6-10 horas para fine-tuning completo de uma categoria

#### Gera√ß√£o de Texto - LLMs Locais
**Modelos Recomendados**:

1. **Mistral 7B** (Recomendado para MVP)
   - Tamanho: ~14GB (fp16), ~7GB (4-bit quantizado)
   - Qualidade: Excelente para descri√ß√µes de produtos
   - Velocidade: ~20-30 tokens/seg em M2 Max
   - Fine-tuning: LoRA ou QLoRA (4-bit)

2. **Llama 3.1 8B**
   - Tamanho: ~16GB (fp16), ~8GB (4-bit)
   - Qualidade: Superior para textos longos
   - Licen√ßa: Permissiva para uso comercial

3. **Phi-3 Mini 3.8B** (Alternativa leve)
   - Tamanho: ~7.5GB (fp16)
   - Velocidade: Mais r√°pido que Mistral
   - Qualidade: Boa para textos curtos

**Configura√ß√£o de Fine-tuning (QLoRA)**:
```python
# Mistral 7B com QLoRA
Base model: mistralai/Mistral-7B-v0.1
Quantization: 4-bit (bitsandbytes)
LoRA config:
  - r: 16
  - lora_alpha: 32
  - lora_dropout: 0.05
  - target_modules: ["q_proj", "v_proj"]

Training params:
  - Batch size: 4 (per device)
  - Gradient accumulation: 4
  - Learning rate: 2e-4
  - Epochs: 3-5
  - Max seq length: 512 tokens
  - Optimizer: paged_adamw_8bit

Memory usage: ~12-16GB (cabe no M2 Max)
```

**Estimativa de Tempo**:
- ~4-6 horas para fine-tuning completo do dataset
- ~8-12 horas com valida√ß√£o e experimenta√ß√£o

#### Embeddings e Modelos Auxiliares
**CLIP para Valida√ß√£o Multimodal**:
```python
# CLIP ViT-L/14 ou ViT-B/32
Model: openai/clip-vit-large-patch14
Memory: ~3GB
Inference: R√°pida em M2 Max (~100 images/sec)
Uso: Valida√ß√£o de consist√™ncia imagem-texto
```

### 1.4.3 Estrat√©gia de Treinamento Eficiente

#### Pipeline de Desenvolvimento Incremental

**Fase 1 - Prototipagem R√°pida - CASUAL SHOES (1 semana)** [PRIORIDADE]:
1. Analisar subset Casual Shoes (~2,845 imagens)
2. Testar SD 1.5 + LoRA com 300-500 imagens
3. Validar pipeline de treinamento MPS
4. Ajustar hiperpar√¢metros para M2 Max
5. Target: FID < 80, tempo de gera√ß√£o < 10s/imagem

**Fase 2 - Fine-tuning Casual Shoes Completo (2 semanas)** [PRIORIDADE]:
1. Treinar SD 1.5 + LoRA em Casual Shoes completo (~2,845 imagens)
2. Gerar dataset expandido (3,000-5,000 imagens sint√©ticas)
3. Integrar CLIP para valida√ß√£o de qualidade
4. Target: FID < 50, CLIP score > 0.25

**Fase 3 - Valida√ß√£o e Refinamento (1 semana)** [PRIORIDADE]:
1. M√©tricas de qualidade (FID, IS, CLIP score)
2. An√°lise visual manual (sample aleat√≥rio)
3. Compara√ß√£o com imagens reais
4. Itera√ß√£o e ajuste fino

**Fase 4 - FUTURO - Expans√£o Multi-Categoria (postponed)**:
1. Repetir processo para Tshirts, Shirts
2. Fine-tune Mistral 7B para descri√ß√µes (postponed)
3. Sistema multi-categoria (postponed)

#### Otimiza√ß√µes Espec√≠ficas para M2 Max

**PyTorch MPS Backend**:
```python
# Configura√ß√£o otimizada
import torch

# Verificar disponibilidade MPS
assert torch.backends.mps.is_available()

# Device setup
device = torch.device("mps")

# Otimiza√ß√µes
torch.mps.set_per_process_memory_fraction(0.8)  # Reservar 80% da RAM
torch.backends.mps.allow_tf32 = True

# Durante treinamento
if epoch % 5 == 0:
    torch.mps.empty_cache()  # Limpar cache periodicamente
```

**Gradient Checkpointing**:
```python
# Reduzir uso de mem√≥ria em 40-50%
model.enable_gradient_checkpointing()
```

**Mixed Precision Training**:
```python
# Acelerar treinamento em 2x
from torch.amp import autocast, GradScaler

scaler = GradScaler()
with autocast(device_type='mps'):
    outputs = model(inputs)
    loss = criterion(outputs, targets)
scaler.scale(loss).backward()
```

### 1.4.4 Limita√ß√µes e Mitiga√ß√µes

**Limita√ß√µes Identificadas**:
1. **Batch Size Reduzido**:
   - Limita√ß√£o: 2-4 imagens por batch vs 8-16 em GPUs dedicadas
   - Mitiga√ß√£o: Gradient accumulation (simular batches maiores)

2. **Velocidade de Treinamento**:
   - Limita√ß√£o: ~2-3x mais lento que GPU NVIDIA A100
   - Mitiga√ß√£o: Treinar overnight, usar checkpoints

3. **Modelos Muito Grandes**:
   - Limita√ß√£o: Modelos 30B+ n√£o cabem na mem√≥ria
   - Mitiga√ß√£o: Focar em modelos 7B-13B com quantiza√ß√£o

4. **Precis√£o Num√©rica**:
   - Limita√ß√£o: MPS ainda tem algumas limita√ß√µes vs CUDA
   - Mitiga√ß√£o: Testar converg√™ncia, usar fp32 se necess√°rio

**Alternativas Cloud (Opcional)**:
- Google Colab Pro (GPU T4/A100): Para experimentos r√°pidos
- Lambda Labs / RunPod: Para treinamento de longa dura√ß√£o
- Manter M2 Max para: Inference, valida√ß√£o, prot√≥tipos

### 1.4.5 Benchmarks Esperados no M2 Max

**Stable Diffusion 1.5 + LoRA**:
- Infer√™ncia: ~4-6 segundos/imagem (512x512, 25 steps)
- Treinamento: ~2.5s/step (batch=2, grad_accum=4)
- 3000 steps: ~2-3 horas

**SDXL + LoRA**:
- Infer√™ncia: ~12-15 segundos/imagem (1024x1024, 25 steps)
- Treinamento: ~6-8s/step (batch=1, grad_accum=8)
- 3000 steps: ~6-8 horas

**Mistral 7B (QLoRA)**:
- Infer√™ncia: ~20 tokens/segundo
- Treinamento: ~3-4s/step (batch=4, grad_accum=4)
- 1 epoch (~11K steps): ~10-12 horas

---

## 2. Fases do Projeto

### Fase 1: Explora√ß√£o e Prepara√ß√£o de Dados (2-3 semanas)
**STATUS**: ‚úÖ EDA Inicial Conclu√≠da | üîÑ Pr√©-processamento Pendente

#### 2.1 An√°lise Explorat√≥ria de Dados (EDA) ‚úÖ CONCLU√çDA
**Objetivo**: Compreender profundamente a estrutura e distribui√ß√£o dos dados

**Status de Implementa√ß√£o**:
- ‚úÖ Scripts de an√°lise criados (`shoes-tranning/exploratory/scripts/`)
- ‚úÖ Notebook interativo criado (`01_initial_eda.ipynb`)
- ‚úÖ An√°lise documentada (ver se√ß√£o 1.3)

**Tarefas Realizadas**:
1. **‚úÖ An√°lise de Imagens**
   - ‚úì Distribui√ß√£o de resolu√ß√µes e aspectos analisada
   - ‚úì Caracter√≠sticas visuais documentadas
   - ‚úì Qualidade e consist√™ncia verificadas
   - ‚úì Identificadas varia√ß√µes de resolu√ß√£o (n√£o padronizadas)
   - **Script**: `image_analysis.py`

2. **‚úÖ An√°lise de Metadados (styles.csv + JSON)**
   - ‚úì Distribui√ß√£o completa de categorias documentada
   - ‚úì 44,446 produtos analisados
   - ‚úì Padr√µes e correla√ß√µes identificados
   - ‚úì Valores faltantes: neglig√≠veis (<0.01%)
   - **Scripts**: `data_summary.py`, `json_metadata_analysis.py`

3. **‚úÖ An√°lise de Texto**
   - ‚úì Comprimento m√©dio de descri√ß√µes: 15-50 palavras
   - ‚úì Estrutura HTML identificada (requer limpeza)
   - ‚úì 3 tipos de descri√ß√µes: description, style_note, materials_care_desc
   - ‚úì Vocabul√°rio rico e espec√≠fico por categoria

**Entreg√°veis Completados**:
- ‚úÖ Notebook de EDA com visualiza√ß√µes (`01_initial_eda.ipynb`)
- ‚úÖ Relat√≥rios de estat√≠sticas descritivas (outputs/)
- ‚úÖ Visualiza√ß√µes geradas (figures/)
- ‚úÖ Dicion√°rio de dados documentado (ver se√ß√£o 1.3)
- ‚úÖ README com instru√ß√µes de uso

**Insights-Chave Obtidos**:
- Dataset de alta qualidade com imagens profissionais
- Top 10 categorias cobrem 57% do dataset
- Tshirts √© a categoria ideal para MVP (~6,200 exemplos)
- Metadados ricos permitem condicionamento multi-atributo
- Fundo limpo/branco ideal para modelos generativos

#### 2.2 Pr√©-processamento
**Objetivo**: Preparar dados para treinamento

**Tarefas**:
1. **Processamento de Imagens**
   ```python
   # Pipeline de preprocessamento
   - Redimensionamento padronizado (256x256, 512x512)
   - Normaliza√ß√£o de valores de pixel
   - Augmenta√ß√£o de dados (rota√ß√£o, flip, crop)
   - Remo√ß√£o de background (opcional, usando segmenta√ß√£o)
   - Cria√ß√£o de embeddings visuais (CLIP, ResNet)
   ```

2. **Processamento de Texto**
   ```python
   # Pipeline de NLP
   - Limpeza e normaliza√ß√£o
   - Tokeniza√ß√£o
   - Remo√ß√£o de stop words (opcional)
   - Cria√ß√£o de embeddings (BERT, GPT)
   ```

3. **Estrutura√ß√£o de Metadados**
   ```python
   # Codifica√ß√£o de categorias
   - Label encoding para categorias hier√°rquicas
   - One-hot encoding para atributos
   - Normaliza√ß√£o de valores num√©ricos (pre√ßo, ano)
   ```

**Entreg√°veis**:
- Scripts de preprocessamento reutiliz√°veis
- Datasets processados e versionados
- Documenta√ß√£o de transforma√ß√µes aplicadas

---

### Fase 2: Desenvolvimento de Modelos Base (4-6 semanas)
**FOCO**: Modelos otimizados para Apple M2 Max (Ver se√ß√£o 1.4)

#### 2.1 Modelo de Gera√ß√£o de Imagens
**Abordagem**: Stable Diffusion 1.5/SDXL com LoRA (Hardware-Optimized)

##### Implementa√ß√£o Principal: SD 1.5 + LoRA (Recomendada para M2 Max)
**Justificativa**:
- Roda nativamente em M2 Max com PyTorch MPS
- LoRA reduz requisitos de mem√≥ria em 90%
- Treinamento vi√°vel em 2-4 horas por categoria
- Qualidade comprovada para produtos de moda

**Arquitetura Otimizada para M2 Max**:
```python
# Configura√ß√£o espec√≠fica para Apple Silicon
Base Model: runwayml/stable-diffusion-v1-5
Fine-tuning Method: LoRA (Low-Rank Adaptation)
LoRA Config:
  - rank: 8-16 (8 para menor mem√≥ria, 16 para melhor qualidade)
  - alpha: 16-32
  - target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

# Condicionamento
Inputs:
  - Categoria: "Tshirt", "Casual Shoes", etc.
  - Atributos: cor, esta√ß√£o, g√™nero
  - Prompt: "A professional product photo of [articleType] in [color], [season] collection, white background"

Output:
  - Imagem 512x512 (SD 1.5)
  - Tempo: 4-6 segundos/imagem
```

**Processo de Treinamento M2 Max-Optimized**:

1. **Setup do Ambiente**:
   ```bash
   # Instalar depend√™ncias otimizadas
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
   pip install diffusers transformers accelerate
   pip install peft bitsandbytes  # Para LoRA

   # Verificar MPS
   python -c "import torch; print(torch.backends.mps.is_available())"
   ```

2. **Prepara√ß√£o de Dados**:
   - Redimensionar imagens para 512x512
   - Criar captions estruturados:
     ```
     "A professional product photo of [Navy Blue Tshirt], Summer collection,
      [Men's casual wear], centered on white background"
     ```
   - Dataset m√≠nimo: 500-1000 imagens para prot√≥tipo
   - Dataset completo: 6200 imagens (Tshirts) para modelo final

3. **Configura√ß√£o de Treinamento**:
   ```python
   # Par√¢metros otimizados para M2 Max (32GB RAM)
   training_args = {
       "learning_rate": 1e-4,
       "batch_size": 2,  # Pequeno devido √† mem√≥ria
       "gradient_accumulation_steps": 8,  # Simula batch=16
       "num_train_epochs": 10,
       "mixed_precision": "fp16",  # MPS suporta fp16
       "gradient_checkpointing": True,  # Economiza 40% mem√≥ria
       "save_steps": 500,
       "eval_steps": 500,
       "logging_steps": 100,
   }

   # Otimiza√ß√µes de mem√≥ria
   - torch.mps.empty_cache() a cada 5 epochs
   - Usar xformers ou attention slicing para reduzir mem√≥ria
   - CPU offloading para CLIP text encoder (opcional)
   ```

4. **T√©cnicas Avan√ßadas (Fase 2.5)**:
   - **ControlNet** (opcional): Manter estrutura/pose consistente
   - **Multiple LoRAs**: Um LoRA por categoria top-5
   - **Textual Inversion**: Aprender tokens espec√≠ficos de produtos

**Estimativas de Performance - Casual Shoes**:
- An√°lise e prepara√ß√£o de dados: ~2-3 horas
- Treinamento prot√≥tipo (300-500 imgs): ~1-1.5 horas
- Treinamento completo (2,845 imgs): ~1.5-2.5 horas
- Gera√ß√£o de 1000 imagens sint√©ticas: ~1.5-2 horas
- Infer√™ncia: 4-6 segundos por imagem
- Mem√≥ria: ~8-12GB durante treinamento

##### Op√ß√£o B: StyleGAN3 (Alternativa)
**Justificativa**: Excelente para produtos com fundo limpo

**Implementa√ß√£o**:
```python
# Configura√ß√£o
Arquitetura: StyleGAN3-T ou StyleGAN3-R
Resolu√ß√£o: 512x512
Conditional: Usar label conditioning ou projection

# Latent space manipulation
- Disentangled representations para atributos
- Style mixing entre categorias
```

##### Op√ß√£o C: DALL-E/Imagen via API (Baseline)
**Justificativa**: Valida√ß√£o r√°pida sem treinamento

**Uso**:
- Gerar baseline para compara√ß√£o
- Avaliar qualidade alcan√ß√°vel
- Testar engineering de prompts

#### 2.2 Modelo de Gera√ß√£o de Texto
**Abordagem**: LLM Local com QLoRA (M2 Max Optimized)

##### Implementa√ß√£o Recomendada: Mistral 7B com QLoRA

**Justificativa**:
- Roda localmente em M2 Max (32GB)
- QLoRA reduz uso de mem√≥ria em 75%
- Qualidade superior para textos de produtos
- Fine-tuning completo em 4-6 horas

**Arquitetura Otimizada para M2 Max**:
```python
# Modelo base
Base: mistralai/Mistral-7B-Instruct-v0.2
Quantization: 4-bit (bitsandbytes)
Method: QLoRA (Quantized Low-Rank Adaptation)

# LoRA Config
LoRA parameters:
  - r: 16 (rank)
  - lora_alpha: 32
  - lora_dropout: 0.05
  - target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  - bias: "none"

# Input format (condicionamento estruturado)
<s>[INST] Generate a product description for:
- Category: Apparel > Topwear > Tshirts
- Color: Navy Blue
- Season: Summer
- Gender: Men
- Brand: Nike
- Additional: Casual, Cotton fabric
[/INST]

# Output esperado
Descri√ß√£o natural, atraente e informativa (50-150 palavras)
```

**Processo de Fine-tuning M2 Max-Optimized**:

1. **Setup do Ambiente**:
   ```bash
   # Instalar depend√™ncias
   pip install transformers accelerate peft bitsandbytes
   pip install datasets trl  # Para SFTTrainer

   # Verificar disponibilidade
   python -c "import torch; print(torch.backends.mps.is_available())"
   ```

2. **Prepara√ß√£o do Dataset**:
   - Extrair descri√ß√µes do dataset (44,424 produtos)
   - Limpar HTML tags das descri√ß√µes
   - Formato instruction-following:
     ```python
     # Template de treinamento
     prompt_template = """<s>[INST] Generate a product description for:
     - Category: {masterCategory} > {subCategory} > {articleType}
     - Color: {baseColour}
     - Season: {season}
     - Gender: {gender}
     - Brand: {brandName}
     - Additional: {usage}, {fabric}
     [/INST]
     {productDescription}</s>"""
     ```
   - Split: 70% treino (31K), 15% val (6.6K), 15% teste (6.6K)

3. **Configura√ß√£o de Treinamento**:
   ```python
   # Par√¢metros otimizados para M2 Max
   from transformers import TrainingArguments

   training_args = TrainingArguments(
       output_dir="./mistral-7b-fashion-qlora",
       num_train_epochs=3,
       per_device_train_batch_size=4,
       gradient_accumulation_steps=4,  # Batch efetivo = 16
       learning_rate=2e-4,
       fp16=False,  # MPS ainda tem issues com fp16 em alguns casos
       bf16=False,
       max_grad_norm=0.3,
       warmup_ratio=0.03,
       lr_scheduler_type="cosine",
       save_strategy="epoch",
       logging_steps=10,
       optim="paged_adamw_8bit",  # Otimizador eficiente
   )

   # QLoRA config
   from peft import LoraConfig

   peft_config = LoraConfig(
       r=16,
       lora_alpha=32,
       lora_dropout=0.05,
       bias="none",
       task_type="CAUSAL_LM",
       target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
   )

   # Quantiza√ß√£o 4-bit
   from transformers import BitsAndBytesConfig

   bnb_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_quant_type="nf4",
       bnb_4bit_compute_dtype=torch.float16,
       bnb_4bit_use_double_quant=True,
   )
   ```

4. **Treinamento**:
   ```python
   from trl import SFTTrainer

   trainer = SFTTrainer(
       model=model,
       train_dataset=train_dataset,
       eval_dataset=val_dataset,
       peft_config=peft_config,
       args=training_args,
       max_seq_length=512,
   )

   # Treinar
   trainer.train()

   # Salvar apenas adaptadores LoRA (~100MB vs 14GB do modelo completo)
   trainer.model.save_pretrained("./fashion-lora-adapter")
   ```

5. **T√©cnicas de Qualidade**:
   - **Temperature**: 0.7-0.9 (maior = mais criativo)
   - **Top-p sampling**: 0.9 (nucleus sampling)
   - **Repetition penalty**: 1.1
   - **Max length**: 150-200 tokens
   - **Validation**: CLIP score para consist√™ncia imagem-texto

**Estimativas de Performance M2 Max**:
- Treinamento (3 epochs): ~4-6 horas
- Infer√™ncia: ~20-30 tokens/segundo (~5-8 segundos/descri√ß√£o)
- Mem√≥ria durante treino: ~14-18GB
- Mem√≥ria durante infer√™ncia: ~8-10GB
- Tamanho do LoRA adapter: ~100-200MB

**Alternativa Mais Leve**: Phi-3 Mini 3.8B
- Treinamento: ~2-3 horas
- Infer√™ncia: ~40-50 tokens/segundo
- Mem√≥ria: ~8GB
- Qualidade: Boa para descri√ß√µes curtas (<100 palavras)

#### 2.3 Modelo de Gera√ß√£o de Metadados
**Abordagem**: VAE ou Normalizing Flows para atributos consistentes

**Implementa√ß√£o**:
```python
# Arquitetura VAE
Encoder: Metadados existentes ‚Üí latent space
Decoder: Latent space ‚Üí metadados novos

# Constraints
- Garantir combina√ß√µes v√°lidas (ex: "Winter Sandals" √© raro)
- Aprender distribui√ß√µes condicionais P(atributos|categoria)
- Usar regras de neg√≥cio quando necess√°rio
```

**Valida√ß√£o**:
- Verificar consist√™ncia l√≥gica (ex: cor branca n√£o deve ter descri√ß√£o "dark tone")
- Checar completude dos campos obrigat√≥rios
- Validar contra schema JSON

---

### Fase 3: Sistema Multimodal Integrado (3-4 semanas)

#### 3.1 Pipeline de Gera√ß√£o Coordenada
**Objetivo**: Garantir coer√™ncia entre imagem, texto e metadados

**Arquitetura do Sistema**:
```
1. Gera√ß√£o de Metadados Base
   ‚Üì
2. Gera√ß√£o de Imagem (condicionada em metadados)
   ‚Üì
3. Gera√ß√£o de Descri√ß√£o (condicionada em metadados + imagem)
   ‚Üì
4. Valida√ß√£o de Consist√™ncia Multimodal
```

**Componente de Valida√ß√£o**:
```python
# CLIP-based consistency checker
- Calcular similaridade CLIP(imagem, descri√ß√£o)
- Threshold m√≠nimo: 0.25-0.30
- Rejeitar ou regenerar se abaixo do threshold

# Attribute verification
- Usar modelo de classifica√ß√£o para verificar atributos na imagem
- Comparar com metadados gerados
```

#### 3.2 Interface de Gera√ß√£o
**Componentes**:
1. **API REST**:
   ```python
   POST /generate/product
   {
     "category": "Shoes",
     "subcategory": "Casual",
     "constraints": {
       "color": "optional",
       "season": "optional"
     },
     "num_samples": 5
   }
   ```

2. **Batch Generation**:
   - Gerar m√∫ltiplos produtos em paralelo
   - Controle de diversidade
   - Export para formato do dataset original

3. **Interactive Web UI**:
   - Gradio ou Streamlit
   - Controles para ajuste de par√¢metros
   - Visualiza√ß√£o de resultados

---

### Fase 4: Avalia√ß√£o e Refinamento (2-3 semanas)

#### 4.1 M√©tricas de Qualidade

##### M√©tricas para Imagens
1. **Qualidade Visual**:
   - FID (Fr√©chet Inception Distance): < 50 (bom), < 30 (excelente)
   - IS (Inception Score): > 5 (bom)
   - LPIPS (perceptual similarity) para diversidade

2. **Consist√™ncia com Categoria**:
   - Accuracy de classificador treinado: > 85%
   - Usar modelo pr√©-treinado (ResNet, EfficientNet)

3. **Avalia√ß√£o Humana**:
   - Realismo (escala 1-5)
   - Adequa√ß√£o √† categoria
   - Qualidade profissional

##### M√©tricas para Texto
1. **Qualidade Lingu√≠stica**:
   - Perplexity: < 30
   - BLEU score vs. descri√ß√µes reais: > 0.3
   - ROUGE scores

2. **Consist√™ncia Factual**:
   - Verificar men√ß√£o de atributos corretos
   - Detec√ß√£o de alucina√ß√µes

3. **Diversidade**:
   - Self-BLEU (deve ser baixo)
   - Unique n-grams ratio

##### M√©tricas Multimodais
1. **CLIP Score**: > 0.25
2. **Image-Text Matching Accuracy**: > 80%
3. **Attribute Consistency Rate**: > 90%

#### 4.2 Estrat√©gias de Refinamento
1. **Iterative Feedback Loop**:
   - Identificar casos de falha
   - Adicionar ao dataset de fine-tuning
   - Re-treinar com √™nfase em casos dif√≠ceis

2. **Ensemble de Modelos**:
   - Combinar outputs de m√∫ltiplos geradores
   - Sele√ß√£o baseada em m√©tricas de qualidade

3. **Human-in-the-Loop**:
   - Curadoria de melhores samples
   - Feedback para melhoria cont√≠nua

---

### Fase 5: Aplica√ß√µes e Deployment (2 semanas)

#### 5.1 Casos de Uso
1. **Data Augmentation**:
   - Balancear categorias sub-representadas
   - Criar varia√ß√µes de produtos existentes

2. **Prototyping de Produtos**:
   - Visualizar novos designs rapidamente
   - Testar conceitos antes de produ√ß√£o

3. **Dataset Sint√©tico para Treinamento**:
   - Treinar modelos de classifica√ß√£o
   - Sistemas de busca visual
   - Sistemas de recomenda√ß√£o

#### 5.2 Deployment
```python
# Stack recomendada
Backend: FastAPI
Model Serving: TorchServe ou NVIDIA Triton
Storage: S3/MinIO para imagens geradas
Database: PostgreSQL para metadados
Queue: Celery + Redis para processamento ass√≠ncrono
```

**Otimiza√ß√µes**:
- Model quantization (INT8)
- Batching de requisi√ß√µes
- Caching de gera√ß√µes comuns
- GPU sharing com vLLM ou TensorRT

---

## 3. Cronograma Estimado

| Fase | Dura√ß√£o | Semanas Acumuladas |
|------|---------|-------------------|
| Fase 1: Explora√ß√£o e Prepara√ß√£o | 2-3 semanas | 3 |
| Fase 2: Desenvolvimento de Modelos | 4-6 semanas | 9 |
| Fase 3: Sistema Multimodal | 3-4 semanas | 13 |
| Fase 4: Avalia√ß√£o e Refinamento | 2-3 semanas | 16 |
| Fase 5: Aplica√ß√µes e Deployment | 2 semanas | 18 |

**Total: 18-20 semanas (4-5 meses)**

---

## 4. Recursos Necess√°rios

### 4.1 Infraestrutura Computacional
**GPU Requirements**:
- M√≠nimo: 1x GPU com 16GB VRAM (RTX 3090/4090, A4000)
- Recomendado: 1x GPU com 24GB+ VRAM (A5000, A6000, RTX 6000)
- Ideal: 2-4x GPUs para treinamento distribu√≠do

**Alternativas Cloud**:
- Google Colab Pro/Pro+ (budget-friendly para experimenta√ß√£o)
- Lambda Labs, RunPod, Vast.ai (GPUs dedicadas)
- AWS/GCP/Azure (produ√ß√£o)

### 4.2 Software e Bibliotecas
```python
# Deep Learning
- PyTorch >= 2.0
- Diffusers (Hugging Face)
- Transformers (Hugging Face)
- PEFT (LoRA, QLoRA)

# Vis√£o Computacional
- OpenCV
- Pillow
- CLIP (OpenAI)
- torchvision

# NLP
- spaCy ou NLTK
- sentence-transformers

# Avalia√ß√£o
- pytorch-fid
- lpips
- torchmetrics

# MLOps
- Weights & Biases ou MLflow
- DVC para versionamento de dados
```

### 4.3 Datasets e Modelos Pr√©-treinados
**Para Download**:
- Fashion Product Images Dataset (Kaggle)
- Stable Diffusion weights (Hugging Face)
- LLaMA 2 ou Mistral weights
- CLIP models (OpenAI)

**Estimativa de Storage**:
- Dataset original: ~15GB (completo) ou 280MB (small)
- Modelos pr√©-treinados: ~20-40GB
- Modelos fine-tuned: ~10-20GB
- Dados processados e cache: ~30-50GB
- **Total: ~100GB m√≠nimo**

---

## 5. Riscos e Mitiga√ß√µes

### 5.1 Riscos T√©cnicos
| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| Qualidade de imagens sint√©ticas insuficiente | M√©dia | Alto | - Testar m√∫ltiplas arquiteturas<br>- Usar modelos estado da arte<br>- Ajustar hyperpar√¢metros extensivamente |
| Inconsist√™ncia multimodal | Alta | M√©dio | - Implementar valida√ß√£o rigorosa<br>- Usar condicionamento forte<br>- Pipeline de gera√ß√£o coordenada |
| Overfitting no dataset pequeno | M√©dia | M√©dio | - Data augmentation<br>- Regulariza√ß√£o<br>- Early stopping |
| Limita√ß√£o computacional | Baixa | Alto | - Usar modelos menores (LoRA)<br>- Cloud GPU on-demand<br>- Gradient accumulation |

### 5.2 Riscos de Projeto
| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| Escopo excessivo | M√©dia | Alto | - MVP com features essenciais<br>- Desenvolvimento iterativo<br>- Prioriza√ß√£o clara |
| Falta de dados de valida√ß√£o | Baixa | M√©dio | - Usar held-out set do dataset original<br>- M√©tricas objetivas complementadas por avalia√ß√£o humana |

---

## 6. Crit√©rios de Sucesso

### 6.1 Crit√©rios M√≠nimos (MVP)
- [ ] Gerar imagens de produtos reconhec√≠veis em 3+ categorias principais
- [ ] FID < 80 para cada categoria
- [ ] Gerar descri√ß√µes coerentes com metadados (verifica√ß√£o manual de 100 samples)
- [ ] Pipeline end-to-end funcional

### 6.2 Crit√©rios Alvo
- [ ] FID < 50 em todas as categorias
- [ ] CLIP score > 0.25 para pares (imagem, descri√ß√£o)
- [ ] Classificador identifica categoria correta em >85% das imagens sint√©ticas
- [ ] Avalia√ß√£o humana: realismo m√©dio > 3.5/5
- [ ] API de gera√ß√£o com lat√™ncia < 30s por produto

### 6.3 Crit√©rios Excel√™ncia
- [ ] FID < 30 (pr√≥ximo de dataset real)
- [ ] Sistema capaz de gerar 1000+ produtos √∫nicos e realistas
- [ ] Integra√ß√£o com sistema de e-commerce real
- [ ] Detec√ß√£o de bias e fairness implementada
- [ ] Documenta√ß√£o completa e c√≥digo reproduz√≠vel

---

## 7. Refer√™ncias e Recursos de Aprendizagem

### 7.1 Papers Fundamentais
1. **Image Generation**:
   - "Denoising Diffusion Probabilistic Models" (Ho et al., 2020)
   - "High-Resolution Image Synthesis with Latent Diffusion Models" (Rombach et al., 2022)
   - "StyleGAN3" (Karras et al., 2021)

2. **Text Generation**:
   - "Language Models are Few-Shot Learners" (GPT-3, Brown et al., 2020)
   - "LoRA: Low-Rank Adaptation of Large Language Models" (Hu et al., 2021)

3. **Multimodal**:
   - "Learning Transferable Visual Models From Natural Language Supervision" (CLIP, Radford et al., 2021)
   - "Flamingo: a Visual Language Model for Few-Shot Learning" (Alayrac et al., 2022)

### 7.2 Tutoriais e Cursos
- Hugging Face Diffusion Models Course
- Fast.ai Deep Learning for Coders
- Stanford CS231n (Computer Vision)
- Stanford CS224n (NLP)

### 7.3 Ferramentas e Frameworks
- [Diffusers Documentation](https://huggingface.co/docs/diffusers)
- [PEFT Library](https://github.com/huggingface/peft)
- [Stable Diffusion Fine-tuning Guide](https://huggingface.co/docs/diffusers/training/overview)

---

## 8. Pr√≥ximos Passos Imediatos

1. **Setup do Ambiente** (Semana 1):
   ```bash
   # Criar ambiente virtual
   python -m venv venv
   source venv/bin/activate

   # Instalar depend√™ncias base
   pip install torch torchvision diffusers transformers
   pip install pandas numpy matplotlib seaborn
   pip install jupyter notebook
   ```

2. **Download do Dataset** (Semana 1):
   - Baixar dataset do Kaggle
   - Organizar estrutura de pastas
   - Verificar integridade dos arquivos

3. **EDA Inicial** (Semana 1-2):
   - Notebook explorando imagens
   - An√°lise de distribui√ß√£o de categorias
   - Estat√≠sticas de metadados

4. **Baseline Model** (Semana 2-3):
   - Implementar classificador simples (para avalia√ß√£o posterior)
   - Testar gera√ß√£o com modelo pr√©-treinado (sem fine-tuning)
   - Estabelecer m√©tricas baseline

---

## 9. Estrutura de C√≥digo Sugerida

```
shoes-training/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Dataset original
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Dados pr√©-processados
‚îÇ   ‚îî‚îÄ‚îÄ synthetic/              # Dados sint√©ticos gerados
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_baseline_models.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_evaluation.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_generator.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_generator.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_diffusion.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_llm.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validators.py
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ       ‚îî‚îÄ‚îÄ generation_pipeline.py
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ diffusion_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ llm_config.yaml
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ planning.md               # Este documento
```

---

## 10. Considera√ß√µes Finais para Aprendizagem

### 10.1 Conceitos-Chave a Dominar
1. **Fundamentos de Deep Learning**:
   - Backpropagation e otimiza√ß√£o (Adam, AdamW)
   - Regulariza√ß√£o (dropout, weight decay)
   - Transfer learning e fine-tuning

2. **Modelos Generativos**:
   - GANs: conceito de gerador vs. discriminador
   - VAEs: latent space e reconstru√ß√£o
   - Diffusion Models: processo de noising e denoising
   - Condicionamento e controle

3. **NLP Moderno**:
   - Transformers e aten√ß√£o
   - Tokeniza√ß√£o e embeddings
   - Fine-tuning eficiente (LoRA, adapters)

4. **Multimodalidade**:
   - Embeddings compartilhados (CLIP)
   - Cross-modal retrieval
   - Consist√™ncia entre modalidades

### 10.2 Abordagem Pedag√≥gica Recomendada
1. **Come√ßar Simples**:
   - Implementar classificador de imagens primeiro
   - Entender o dataset profundamente
   - Usar modelos pequenos para experimenta√ß√£o r√°pida

2. **Progress√£o Gradual**:
   - Baseline ‚Üí Fine-tuning ‚Üí Custom Architectures
   - Single modality ‚Üí Multimodal integration
   - M√©todos supervisionados ‚Üí Generative models

3. **Experimenta√ß√£o Ativa**:
   - Testar hip√≥teses com experimentos controlados
   - Manter log detalhado de resultados (W&B)
   - Analisar falhas para aprendizado

4. **Documenta√ß√£o Cont√≠nua**:
   - Comentar c√≥digo extensivamente
   - Criar notebooks explicativos
   - Escrever relat√≥rios de progresso semanais

---

**√öltima Atualiza√ß√£o**: 2025-10-26
**Vers√£o**: 1.0
**Autor**: Planejamento gerado para fins de aprendizagem e pesquisa acad√™mica
